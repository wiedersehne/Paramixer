config = {
    "encode":{
        "Paramixer":{
            "name":"paramixer",
            "vocab_size": 4 + 1, # 5 unique symbols + 1 PAD
            "embedding_size": 32,
            "max_seq_len": 16384,
            "n_W": 14,
            "n_class": 2,
            "pooling_type": "FLATTEN", # "FLATTEN" or "CLS"
            "head": ['linear'], # ['linear'] or ['non-linear', 32], the second value is the number of hidden neurons
            "use_cuda": True,
            "use_residuals": True,
            "dropout1_p": 0.2,
            "dropout2_p": 0.8,
            "init_embedding": False,
            "pos_embedding": ['APC'],
            "problem": "encode",
            "protocol": "chord",
            'n_layers': 1,
            "hidden_size":128
        },
        "Linformer":{
            "name": "linformer",
            "vocab_size": 5,
            "max_seq_len": 16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "Performer":{
            "name": "performer",
            "vocab_size": 5,
            "max_seq_len": 16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "Transformer":{
            "name": "transformer",
            "vocab_size": 5,
            "max_seq_len": 16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 4,
            "heads": 4,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "Nystromformer":{
            "name": "nystromformer",
            "vocab_size": 5,
            "max_seq_len": 16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "LStransformer": {
            "name": "lstransformer",
            "vocab_size": 5,
            "max_seq_len": 16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "Reformer": {
            "name": "reformer",
            "vocab_size": 5,
            "max_seq_len": 16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "training":{
            "device_id": 0,
            "batch_size":16,
            "learning_rate":0.0001,
            "eval_frequency":1,
            "num_train_steps":200
        },
    },
    "ensembl":{
        "Paramixer":{
            "name":"paramixer",
            "vocab_size": 5 + 1, # 5 unique symbols + 1 PAD
            "embedding_size": 32,
            "max_seq_len":16384,
            "n_W": 14,
            "n_class": 2,
            "pooling_type": "FLATTEN", # "FLATTEN" or "CLS"
            "head": ['linear'], # ['linear'] or ['non-linear', 32], the second value is the number of hidden neurons
            "use_cuda": True,
            "use_residuals": True,
            "dropout1_p": 0.2,
            "dropout2_p": 0.8,
            "init_embedding": False,
            "pos_embedding": ['APC'],
            "problem": "ensembl",
            "protocol": "chord",
            'n_layers': 1,
            "hidden_size":128
        },
        "Linformer":{
            "name": "linformer",
            "vocab_size": 6,
            "max_seq_len":16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "ensembl"
        },
        "Performer":{
            "name": "performer",
            "vocab_size": 6,
            "max_seq_len":16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "encode"
        },
        "Transformer":{
            "name": "transformer",
            "vocab_size": 6,
            "max_seq_len":16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "ensembl"
        },
        "Nystromformer":{
            "name": "nystromformer",
            "vocab_size": 5,
            "max_seq_len":16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "ensembl"
        },
        "LStransformer": {
            "name": "lstransformer",
            "vocab_size": 5,
            "max_seq_len":16384,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 4,
            "heads": 4,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "ensembl"
        },
        "Reformer": {
            "name": "reformer",
            "vocab_size": 5,
            "n_vec": 128,
            "add_init_linear_layer": False,
            "dim": 32,
            "depth": 1,
            "heads": 1,
            "pooling_type": "FLATTEN",
            "head": ['linear'],
            "n_class": 2,
            "use_cuda": True,
            "pos_embedding": ['APC'],
            "problem": "ensembl"
        },
        "training":{
            "device_id": 0,
            "batch_size":16,
            "learning_rate":0.0001,
            "eval_frequency":1,
            "num_train_steps":200
        }
    }
}

